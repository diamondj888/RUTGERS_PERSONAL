{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Who are the Bossy Words?\n","\n"," On this activity you will use TF-IDF to find the most relevant words on news articles that talk about money in the [Reuters Corpus](https://www.nltk.org/book/ch02.html#reuters-corpus) bundled in `NLTK`. Once you find the most relevant words, you should create a word cloud."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# initial imports\n","import nltk\n","from nltk.corpus import reuters\n","import numpy as np\n","import pandas as pd\n","#from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","plt.style.use(\"seaborn-whitegrid\")\n","mpl.rcParams[\"figure.figsize\"] = [20.0, 10.0]\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Loading the Reuters Corpus\n","\n"," The first step is to load the Reuters Corpus."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package reuters to\n","[nltk_data]     /Users/jasonraeppold/nltk_data...\n","[nltk_data]   Package reuters is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Download/update the Reuters dataset\n","nltk.download(\"reuters\")\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Getting the News About Money\n","\n"," You will analyze only news that talk about _money_. There are two categories on the Reuters Corpus that talk about money: `money-fx` and `money-supply`. In this section, you will filter the news by these categories.\n","\n"," Take a look into the [Reuters Corpus documentation](https://www.nltk.org/book/ch02.html#reuters-corpus) and check how you can retrieve the categories of a document using the `reuters.categories()` method; write some lines of code to retrieve all the news articles that are under the `money-fx` or the `money-supply` categories.\n","\n"," **Hint:**\n"," You can use a comprehension list or a for-loop to accomplish this task."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Getting all documents ids under the money-fx and money-supply categories\n","categories = [\"money-fx\", \"money-supply\"]\n","all_docs_id = reuters.fileids()\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["VIEILLE MONTAGNE REPORTS LOSS, DIVIDEND NIL\n","  1986 Year\n","      Net loss after exceptional charges 198 mln francs vs profit\n","      250 mln\n","      Exceptional provisions for closure of Viviez electrolysis\n","      Plant 187 mln francs vs exceptional gain 22 mln\n","      Sales and services 16.51 billion francs vs 20.20 billion\n","      Proposed net dividend on ordinary shares nil vs 110 francs\n","      Company's full name is Vieille Montagne SA &lt;VMNB.BR>.\n","  \n","\n","\n"]}],"source":["# Creating the working corpus containing the text from all the news articles about money\n","corpus_id = all_docs_id[0:]\n","corpus = [reuters.raw(doc) for doc in corpus_id]\n","# Printing a sample article\n","print(corpus[25])"]},{"cell_type":"markdown","metadata":{},"source":[" ## Calculating the TF-IDF Weights\n","\n"," Calculate the TF-IDF weight for each word on the working corpus using the `TfidfVectorizer()` class. Remember to include the `stop_words='english'` parameter."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Calculating TF-IDF for the working corpus.\n","vectorizer = TfidfVectorizer(stop_words=\"english\")\n","X_corpus = vectorizer.fit_transform(corpus)\n"]},{"cell_type":"markdown","metadata":{},"source":[" Create a DataFrame representation of the TF-IDF weights of each term in the working corpus. Use the `sum(axis=0)` method to calculate a measure similar to the term frequency based on the TF-IDF weight, this value will be used to rank the terms for the word cloud creation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a DataFrame Representation of the TF-IDF results\n","\n","# Order the DataFrame by word frequency in descending order\n","\n","# Print the top 10 words\n","money_news_df.head(10)\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Retrieving the Top Words\n","\n"," In order to create the word cloud you should get the top words, in this case we will use a thumb rule that has been empirically tested by some NLP experts that states that words with a frequency between 10 and 30 might be the most relevant in a corpus.\n","\n"," Following this rule, create a new DataFrame containing only those words with the mentioned frequency."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Top words will be those with a frequency between 10 ans 30 (thumb rule)\n","\n","\n","top_words.head(10)\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Creating Word Cloud\n","\n"," Now you have all the pieces needed to create a word cloud based on TF-IDF weights, so use the `WordCloud` library to create it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a string list of terms to generate the word cloud\n","terms_list = str(top_words[\"Word\"].tolist())\n","\n","# Create the word cloud\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Challenge: Looking for Documents that Contains Top Words\n","\n"," Finally you might find interesting to search those articles that contain the most relevant words. Create a function called `retrieve_docs(terms)` that receive a list of terms as parameter and extract from the working corpus all those news articles that contains the search terms. On this function you should use the `reuters.words()` method to retrieve the tokenized version of each article as can be seen on the [Reuters Corpus documentation](https://www.nltk.org/book/ch02.html#reuters-corpus).\n","\n"," **Hint:** To find any occurrence of the search terms you might find quite useful [this post on StackOverflow](https://stackoverflow.com/a/25102099/4325668), also you should lower case all the words to ease your terms search."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def retrieve_docs(terms):\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### Question 1: How many articles talk about Yen?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(retrieve_docs([\"yen\"]))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Question 2: How many articles talk about Japan or Banks?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(retrieve_docs([\"japan\", \"banks\"]))\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### Question 3: How many articles talk about England or Dealers?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(retrieve_docs([\"england\", \"dealers\"]))\n"]}],"metadata":{"file_extension":".py","interpreter":{"hash":"7631c39f756ee97fc32a5db05973838915965d11307db0fdf357e8b46bea8845"},"kernelspec":{"display_name":"Python 3.7.11 ('dev')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","orig_nbformat":2,"pygments_lexer":"ipython3","version":3},"nbformat":4,"nbformat_minor":2}
